---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel
  namespace: observability-backend
spec:
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.103.0
  mode: deployment
  replicas: 1
  ports:
    - port: 8888
      protocol: TCP
      name: metrics
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch: {}

    connectors:
      spanmetrics: {}

    exporters:
      loadbalancing:
        routing_key: traceID
        protocol:
          otlp:
            tls:
              insecure: true
            sending_queue:
              queue_size: 4000
        resolver:
          dns:
            hostname: sampling-collector-headless.observability-backend.svc.cluster.local
            port: 4317

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [loadbalancing]
---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: sampling
  namespace: observability-backend
spec:
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.103.0
  mode: deployment
  replicas: 3
  ports:
    - port: 8888
      protocol: TCP
      name: metrics
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch: {}
      # 1. Sample 100% of traces with ERROR-ing spans
      # 2. Sample 100% of trace which have a duration longer than 500ms
      # 3. Randomized sampling of 10% of traces without errors and latencies.
      tail_sampling:
        decision_wait: 10s # time to wait before making a sampling decision
        num_traces: 100 # number of traces to be kept in memory
        expected_new_traces_per_sec: 10 # expected rate of new traces per second
        policies:
          - name: keep-errors
            type: status_code
            status_code: {status_codes: [ERROR]}
         # - name: keep-slow-traces
         #   type: latency
         #   latency: {threshold_ms: 500}
         # - name: randomized-policy
         #   type: probabilistic
         #   probabilistic: {sampling_percentage: 10} 

    connectors:
      spanmetrics: {}

    exporters:
      otlp/traces:
        endpoint: jaeger-collector:4317
        tls:
          insecure: true

      otlphttp/metrics:
        endpoint: http://prometheus:80/api/v1/otlp/
        tls:
          insecure: true

      debug:
        verbosity: detailed

    service:
      pipelines:
        traces/tailsampling:
          receivers: [otlp]
          processors: [tail_sampling, batch]
          exporters: [otlp/traces]
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [spanmetrics]
        metrics:
          receivers: [spanmetrics, otlp]
          processors: [batch]
          exporters: [otlphttp/metrics, debug]
        logs:
          receivers: [otlp]
          processors: [batch]
          exporters: [debug]
